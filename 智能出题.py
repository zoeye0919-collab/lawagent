import os
import json
from langchain_openai import ChatOpenAI
from langchain_core.prompts import ChatPromptTemplate
from langchain_core.output_parsers import JsonOutputParser
from langchain_community.document_loaders import PyPDFLoader, Docx2txtLoader
from pydantic import BaseModel, Field
from typing import List, Optional

# --- 1. é…ç½®åŒºåŸŸ ---
# è¯·ç¡®ä¿è¿™é‡Œå¡«å†™äº†ä½ çš„ Key
api_key = "sk-d75143a2504f43089e4c20d2db3a3a52"
os.environ["DASHSCOPE_API_KEY"] = api_key

EXAM_DATA_DIR = "./æ³•è€ƒçœŸé¢˜"  # é¢˜åº“æºæ–‡ä»¶ç›®å½•
EXAM_DB_FILE = "exam_db.json"  # è¾“å‡ºæ–‡ä»¶

# åˆå§‹åŒ–æ¨¡å‹
llm = ChatOpenAI(
    api_key=api_key,
    base_url="https://dashscope.aliyuncs.com/compatible-mode/v1",
    model="qwen-max",
    temperature=0.1,
)


# --- 2. å®šä¹‰æ•°æ®ç»“æ„ ---
class ExamQuestion(BaseModel):
    subject: str = Field(description="é¢˜ç›®æ‰€å±çš„æ³•å¾‹å­¦ç§‘ï¼Œå¿…é¡»ä»ä»¥ä¸‹åˆ—è¡¨ä¸­é€‰æ‹©ä¸€ä¸ªï¼šåˆ‘æ³•, æ°‘æ³•, è¡Œæ”¿æ³•, åˆ‘äº‹è¯‰è®¼æ³•, æ°‘äº‹è¯‰è®¼æ³•, å•†ç»æ³•, ç†è®ºæ³•, ä¸‰å›½æ³•")
    question_text: str = Field(description="é¢˜ç›®å®Œæ•´çš„é¢˜å¹²å†…å®¹")
    options: List[str] = Field(description="é€‰é¡¹åˆ—è¡¨ï¼Œä¾‹å¦‚ ['A. ...', 'B. ...']")
    correct_answer: Optional[str] = Field(description="æ­£ç¡®ç­”æ¡ˆï¼Œä¾‹å¦‚ 'A' æˆ– 'ABCD'ï¼Œå¦‚æœæ–‡ä¸­æ²¡æœ‰åˆ™ç•™ç©º")
    analysis: Optional[str] = Field(description="é¢˜ç›®è§£æï¼Œå¦‚æœæ–‡ä¸­æ²¡æœ‰åˆ™ç•™ç©º")


class QuestionList(BaseModel):
    questions: List[ExamQuestion]


# --- 3. æ ¸å¿ƒæå–é€»è¾‘ ---
def extract_questions_from_file(file_path):
    print(f"ğŸ“„ æ­£åœ¨å¤„ç†æ–‡ä»¶: {os.path.basename(file_path)} ...")
    ext = os.path.splitext(file_path)[1].lower()

    loader = None
    if ext == ".pdf":
        loader = PyPDFLoader(file_path)
    elif ext == ".docx":
        loader = Docx2txtLoader(file_path)
    else:
        print(f"âš ï¸ è·³è¿‡ä¸æ”¯æŒçš„æ–‡ä»¶æ ¼å¼: {file_path}")
        return []

    try:
        docs = loader.load()
        full_text = "\n".join([d.page_content for d in docs])

        # æ³¨æ„ï¼šæ­¤å¤„æˆªå–å‰15000å­—ã€‚å¦‚æœæ–‡æ¡£å¾ˆé•¿ï¼Œå»ºè®®åœ¨ç”Ÿäº§ç¯å¢ƒä¸­åšåˆ‡ç‰‡å¾ªç¯
        text_chunk = full_text[:15000]

        parser = JsonOutputParser(pydantic_object=QuestionList)
        prompt = ChatPromptTemplate.from_messages([
            ("system", """ä½ æ˜¯ä¸€ä¸ªä¸“ä¸šçš„æ³•è€ƒæ•°æ®å¤„ç†ä¸“å®¶ã€‚è¯·ä»æ–‡æ¡£ä¸­æå–çœŸé¢˜ï¼Œå¹¶ä¸ºæ¯ä¸€é“é¢˜è¿›è¡Œå­¦ç§‘åˆ†ç±»ã€‚
                    è¯·ä¸¥æ ¼éµå®ˆä»¥ä¸‹è§„åˆ™ï¼š
                    1. **è‡ªåŠ¨åˆ†ç±»**ï¼šæ ¹æ®é¢˜ç›®å†…å®¹ï¼Œåˆ¤æ–­å…¶å±äºä»¥ä¸‹å“ªä¸ªå­¦ç§‘ï¼š[åˆ‘æ³•, æ°‘æ³•, è¡Œæ”¿æ³•, åˆ‘äº‹è¯‰è®¼æ³•, æ°‘äº‹è¯‰è®¼æ³•, å•†ç»æ³•, ç†è®ºæ³•, ä¸‰å›½æ³•]ã€‚
                    2. **æ ¼å¼è§„èŒƒ**ï¼šè¾“å‡ºæ ‡å‡†çš„ JSON æ ¼å¼ã€‚
                    3. **å¤„ç†ç¼ºå¤±**ï¼šå¦‚æœæ–‡æ¡£ä¸­æ²¡æœ‰ç­”æ¡ˆæˆ–è§£æï¼Œå¯¹åº”å­—æ®µç•™ç©ºå­—ç¬¦ä¸²ã€‚
                    {format_instructions}"""),
            ("human", "ã€æ–‡æ¡£å†…å®¹ç‰‡æ®µã€‘:\n{text}")
        ])

        chain = prompt | llm | parser
        result = chain.invoke({
            "text": text_chunk,
            "format_instructions": parser.get_format_instructions()
        })
        questions = result.get('questions', [])
        print(f"æˆåŠŸæå– {len(questions)} é“é¢˜ç›®")
        return questions
    except Exception as e:
        print(f"âŒ æå–å¤±è´¥: {e}")
        return []


def main():
    if not os.path.exists(EXAM_DATA_DIR):
        os.makedirs(EXAM_DATA_DIR)
        print(f"ğŸ“ å·²åˆ›å»ºæ–‡ä»¶å¤¹ {EXAM_DATA_DIR}ï¼Œè¯·å°† PDF/Word æ”¾å…¥å…¶ä¸­åå†æ¬¡è¿è¡Œã€‚")
        return

    files = [f for f in os.listdir(EXAM_DATA_DIR) if f.endswith(('.pdf', '.docx'))]
    if not files:
        print(f"âš ï¸ {EXAM_DATA_DIR} æ–‡ä»¶å¤¹ä¸ºç©ºï¼Œè¯·æ”¾å…¥çœŸé¢˜æ–‡ä»¶ã€‚")
        return

    global_db = {}

    print(f"ğŸš€ å¼€å§‹æ„å»ºæ™ºèƒ½é¢˜åº“ï¼Œå…± {len(files)} ä¸ªæ–‡ä»¶...")

    for f in files:
        full_path = os.path.join(EXAM_DATA_DIR, f)

        # æå–é¢˜ç›®
        extracted_qs = extract_questions_from_file(full_path)

        for q in extracted_qs:
            # ä»¥æ­¤å¤„æå–çš„ subject ä¸ºå‡†ï¼Œæ¸…æ´—ä¸€ä¸‹å¯èƒ½çš„æ ¼å¼æ‚ä¹±
            sub_name = q.get('subject', 'ç»¼åˆçŸ¥è¯†').strip()

            # ç®€å•çš„å½’ä¸€åŒ–å¤„ç†
            if "åˆ‘æ³•" in sub_name:
                sub_name = "åˆ‘æ³•"
            elif "æ°‘æ³•" in sub_name:
                sub_name = "æ°‘æ³•"
            elif "è¡Œæ”¿" in sub_name:
                sub_name = "è¡Œæ”¿æ³•"
            elif "åˆ‘è¯‰" in sub_name or "åˆ‘äº‹è¯‰è®¼" in sub_name:
                sub_name = "åˆ‘äº‹è¯‰è®¼æ³•"
            elif "æ°‘è¯‰" in sub_name or "æ°‘äº‹è¯‰è®¼" in sub_name:
                sub_name = "æ°‘äº‹è¯‰è®¼æ³•"

            if sub_name not in global_db:
                global_db[sub_name] = []

            # ç§»é™¤ subject å­—æ®µæœ¬èº«å†å­˜å…¥ï¼ˆå¯é€‰ï¼Œä¸ºäº† JSON å¹²å‡€ï¼‰
            # q.pop('subject', None)
            global_db[sub_name].append(q)

    # ä¿å­˜
    with open(EXAM_DB_FILE, "w", encoding='utf-8') as f:
        json.dump(global_db, f, ensure_ascii=False, indent=2)

    # æ‰“å°ç»Ÿè®¡ä¿¡æ¯
    print("\nğŸ‰ é¢˜åº“æ„å»ºå®Œæˆï¼åˆ†ç±»ç»Ÿè®¡å¦‚ä¸‹ï¼š")
    for sub, qs in global_db.items():
        print(f"   - {sub}: {len(qs)} é¢˜")
    print(f"æ–‡ä»¶å·²ä¿å­˜è‡³: {EXAM_DB_FILE}")


if __name__ == "__main__":
    main()