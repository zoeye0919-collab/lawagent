import streamlit as st
import os
import tempfile
import time
import json
import random
from docx import Document
from langchain_openai import ChatOpenAI
from langchain_core.prompts import ChatPromptTemplate
from langchain_community.embeddings import DashScopeEmbeddings
from langchain_community.vectorstores import FAISS
from langchain_community.document_loaders import PyPDFLoader, Docx2txtLoader,TextLoader
from langchain_text_splitters import RecursiveCharacterTextSplitter
from langchain_classic.chains import create_retrieval_chain
from langchain_community.tools import DuckDuckGoSearchRun


# åˆå§‹åŒ–é…ç½®

api_key = "sk-d75143a2504f43089e4c20d2db3a3a52"
os.environ["DASHSCOPE_API_KEY"] = api_key

EXAM_DATA_DIR = "./æ³•è€ƒçœŸé¢˜"
EXAM_DB_FILE = "exam_db.json"

llm = ChatOpenAI(
    api_key=api_key,
    base_url="https://dashscope.aliyuncs.com/compatible-mode/v1",
    model="qwen-max",
    temperature=0.1,
)

embeddings = DashScopeEmbeddings(model="text-embedding-v4")

# åˆå§‹åŒ–æœç´¢å·¥å…·
search_tool = DuckDuckGoSearchRun()


# --- 2. åŠ è½½çŸ¥è¯†åº“ ---
# æ³•å¾‹æ¡æ–‡çŸ¥è¯†åº“
@st.cache_resource
def load_db():
    if os.path.exists("legal_faiss_index"):
        return FAISS.load_local("legal_faiss_index", embeddings, allow_dangerous_deserialization=True)
    return None


vectorstore = load_db()

# æ³•å¾‹è‹±æ–‡æœ¯è¯­çŸ¥è¯†åº“
@st.cache_resource
def load_term_db():
    if os.path.exists("term_faiss_index"):
        return FAISS.load_local("term_faiss_index", embeddings, allow_dangerous_deserialization=True)
    return None

term_vectorstore = load_term_db()

# --- 3. æ ¸å¿ƒåŠŸèƒ½å‡½æ•° ---
# åŠŸèƒ½ä¸€ï¼šæ³•å¾‹æ–‡æœ¬ç¿»è¯‘
def legal_translation(text):
    prompt = ChatPromptTemplate.from_messages([
        ("system", "ä½ æ˜¯ä¸€åç²¾é€šä¸­å›½æ³•å¾‹ä¸æ™®é€šæ³•ç³»çš„èµ„æ·±ç¿»è¯‘ä¸“å®¶ã€‚è¯·å°†ç”¨æˆ·è¾“å…¥çš„æ–‡æœ¬è¿›è¡Œä¸“ä¸šæ³•å¾‹ç¿»è¯‘ã€‚\n"
                   "- å¦‚æœæ˜¯ä¸­æ–‡ï¼Œè¯‘ä¸ºè‹±æ–‡ã€‚\n"
                   "- å¦‚æœæ˜¯è‹±æ–‡ï¼Œè¯‘ä¸ºä¸­æ–‡ã€‚\n"
                   "- é‡ç‚¹ï¼šç¡®ä¿'Consideration(å¯¹ä»·)', 'Performance(å±¥è¡Œ)', 'Third Party(ç¬¬ä¸‰äºº)'ç­‰æœ¯è¯­å‡†ç¡®ã€‚"),
        ("human", "{input}")
    ])
    chain = prompt | llm
    return chain.invoke({"input": text}).content

# åŠŸèƒ½äºŒï¼šæ¡ˆä¾‹åˆ†æ
def case_analysis(case_text):
    context_text = ""
    if vectorstore:
        retriever = vectorstore.as_retriever(search_kwargs={"k": 3})
        docs = retriever.invoke(case_text)
        context_text = "\n\n".join([d.page_content for d in docs])

    prompt = ChatPromptTemplate.from_messages([
        ("system", "ä½ æ˜¯ä¸€åé«˜çº§æ³•å¾‹é¡¾é—®ã€‚è¯·æ ¹æ®ç”¨æˆ·æä¾›çš„ã€æ¡ˆæƒ…äº‹å®ã€‘ï¼Œç»“åˆã€å‚è€ƒèµ„æ–™ã€‘ï¼ˆå¦‚æœæœ‰ï¼‰ï¼Œæ’°å†™ä¸€ä»½ç»“æ„åŒ–çš„åˆ†ææŠ¥å‘Šã€‚\n"
                   "å¯¹æ¡ˆä»¶ä¸­çš„æ¯ä¸ªäº‰è®®ç„¦ç‚¹åˆ†åˆ«åˆ†æï¼Œè¦æœ‰å¯¹åº”çš„æ³•æ¡é€‚ç”¨ã€ç»“è®ºå»ºè®®ã€‚\n"
                   "ä»…å›´ç»•æ¡ˆæƒ…ä¸­å®¢è§‚å‘ˆç°çš„äº‹å®è¿›è¡Œæ³•å¾‹è¯„ä»·ï¼Œä¸å¾—æ“…è‡ªè¡¥å……æˆ–å‡è®¾ä¸å­˜åœ¨çš„äº‹å®ã€‚\n"
                   "æŠ¥å‘Šæ ¼å¼è¦æ±‚ï¼š\n"
                   "1. **æ¡ˆä»¶èƒŒæ™¯**ï¼šå¯¹ç”¨æˆ·ä¸Šä¼ çš„æ¡ˆä»¶åšç®€å•æ¦‚æ‹¬ï¼Œ200-300å­—å·¦å³ã€‚\n"
                   "2. **äº‰è®®ç„¦ç‚¹**ï¼šå½’çº³æ ¸å¿ƒæ³•å¾‹é—®é¢˜ï¼Œè¦æ±‚ç‚¹æ˜æ„æˆä»€ä¹ˆä¾µæƒè¡Œä¸º/æ„æˆä»€ä¹ˆç½ªåã€‚\n"
                   "3. **æ³•æ¡é€‚ç”¨**ï¼šå¼•ç”¨æœ€ç›¸å…³çš„æ³•å¾‹æ¡æ¬¾ï¼Œç»™å‡ºæ³•æ¡å‡ºå¤„ã€‚\n"
                   "4. **ç»“è®ºå»ºè®®**ï¼šç»™å‡ºå…·ä½“çš„æ“ä½œå»ºè®®ã€‚\n "
                   "ç¤ºä¾‹ï¼š\n "
                   "1.**æ¡ˆä»¶èƒŒæ™¯**ï¼šæŸæˆ¿åœ°äº§å¼€å‘é¡¹ç›®ä½äºæˆ‘å›½æŸä¸€çº¿åŸå¸‚ï¼Œå¼€å‘å•†ä¸ºæŸæˆ¿åœ°äº§å¼€å‘æœ‰é™å…¬å¸(ä»¥ä¸‹ç®€ç§°â€œå¼€å‘å•†â€)ï¼Œé¡¹ç›®åä¸ºâ€œæŸæ¹¾èŠ±å›­â€è¯¥é¡¹ç›®å åœ°çº¦1000äº©ï¼Œæ€»å»ºç­‘é¢ç§¯çº¦200ä¸‡å¹³æ–¹ç±³ï¼Œ\n "
                   "åŒ…æ‹¬ä½å®…ã€å•†ä¸šã€åŠå…¬ç­‰å¤šç§ä¸šæ€ï¼Œé¡¹ç›®è‡ª2008å¹´å¼€å§‹å»ºè®¾ï¼Œé¢„è®¡åµ¯éƒ¨é¢ç­‰13å¹´ç«£å·¥ã€‚ç„¶è€Œï¼Œåœ¨é¡¹ç›®æ–½å·¥è¿‡ç¨‹ä¸­ï¼Œå¼€å‘å•†ä¸éƒ¨åˆ†ä¸šä¸»å°±æˆ¿å±‹è´¨é‡é—®é¢˜äº§ç”Ÿäº†çº çº·ï¼Œè¿›é¢å¼•å‘äº†è¯‰è®¼ã€‚\n"
                   "2.**äº‰è®®ç„¦ç‚¹**ï¼š1.æˆ¿å±‹è´¨é‡é—®é¢˜ \nä¸šä¸»è®¤ä¸ºï¼Œæˆ¿å±‹å­˜åœ¨ä»¥ä¸‹è´¨é‡é—®é¢˜:\n(1)å¢™ä½“è£‚ç¼:ä¸šä¸»åæ˜ ï¼Œéƒ¨åˆ†å¢™ä½“å‡ºç°è£‚ç¼ï¼Œè£‚ç¼é•¿åº¦ä¸ä¸€ï¼Œå®½åº¦ä»å‡ æ¯«ç±³åˆ°å‡ å˜ç±³ä¸ç­‰(2)æ¸—æ°´é—®é¢˜:ä¸šä¸»åæ˜ ï¼Œéƒ¨åˆ†æˆ¿å±‹å­˜åœ¨æ¸—æ°´ç°è±¡ï¼Œå°¤å…¶åœ¨é›¨å¤©æ›´ä¸ºä¸¥é‡ã€‚\n"
                   "3.**æ³•æ¡é€‚ç”¨**ï¼š1.æˆ¿å±‹è´¨é‡é—®é¢˜:å…³äºæˆ¿å±‹è´¨é‡é—®é¢˜ï¼Œæ ¹æ®ã€Šä¸­åäººæ°‘å…±å’Œå›½å»ºç­‘æ³•ã€‹å’Œã€Šå»ºè®¾å·¥ç¨‹è´¨é‡ç®¡ç†æ¡ä¾‹ã€‹çš„ç›¸å…³è§„å®šï¼Œå¼€å‘å•†åº”ä¿è¯æˆ¿å±‹è´¨é‡ç¬¦å”–åˆå›½å®¶æ ‡å‡†ï¼Œæœ¬æ¡ˆä¸­ï¼Œä¸šä¸»åæ˜ çš„å¢™ä½“è£‚ç¼ã€æ¸—æ°´ç­‰é—®é¢˜ã€ç»é‰´å®šï¼Œç¡®å±æˆ¿å±‹è´¨é‡é—®é¢˜ã€‚å¯¹æ­¤ï¼Œå¼€å‘å•†åº”æ‰¿æ‹…ç›¸åº”çš„æ³•å¾‹è´£ä»»ã€‚\n"
                   "4.**ç»“è®ºå»ºè®®**ï¼š1.å¯¹ä¸šä¸»åæ˜ çš„æˆ¿åŸè´¨é‡é—®é¢˜ï¼Œå¼€å‘å•†åº”è´Ÿè´£ä¿®å¤ï¼Œä¿®å¤è´¹ç”¨ç”±å¼€å‘å•†æ‰¿æ‹…ã€‚"),
        ("human", "ã€æ¡ˆæƒ…äº‹å®ã€‘ï¼š{input}\n\nã€å‚è€ƒèµ„æ–™ã€‘ï¼š{context}")
    ])
    chain = prompt | llm
    return chain.invoke({"input": case_text, "context": context_text}).content

# åŠŸèƒ½ä¸‰ï¼šæ³•å¾‹é—®ç­”
def smart_qa_search(query, vectorstore):
    if not vectorstore:
        return run_web_search(query)

    # 1. æ£€ç´¢æœ¬åœ°çŸ¥è¯†
    retriever = vectorstore.as_retriever(search_kwargs={"k": 5})
    docs = retriever.invoke(query)

    # 2. æ„é€ å¸¦æ¥æºçš„ä¸Šä¸‹æ–‡
    context_items = []
    for i, d in enumerate(docs):
        source = os.path.basename(d.metadata.get("source", "æœªçŸ¥æ³•æ¡"))
        context_items.append(f"ã€æ¡æ–‡ä¾æ® {i+1}ã€‘(å‡ºå¤„: {source}):\n{d.page_content}")

    context_text = "\n\n".join(context_items)

    # 3. å¼ºåŒ–æ³•å¾‹å­¦ä¹ ä¸æ¨ç†çš„æç¤ºè¯
    qa_prompt = ChatPromptTemplate.from_messages([
        ("system", """ä½ æ˜¯ä¸€åæå…¶ä¸¥è°¨çš„ä¸­å›½æ³•å¾‹ä¸“å®¶ã€‚è¯·åŸºäºæä¾›çš„ã€æ³•å¾‹æ¡æ–‡ä¾æ®ã€‘å›ç­”ç”¨æˆ·é—®é¢˜ï¼Œå¹¶åŠ å…¥è‡ªå·±çš„æ¨ç†æ€è€ƒä¸ç†è§£ã€‚
        
è¯·éµå¾ªä»¥ä¸‹å›ç­”å‡†åˆ™ï¼š
1. **æ³•æ¡ä¼˜å…ˆ**ï¼šå¿…é¡»ä¼˜å…ˆä½¿ç”¨æä¾›çš„æ¡æ–‡å†…å®¹ã€‚å›ç­”æ—¶è¯·æ˜ç¡®æŒ‡å‡ºå¼•ç”¨äº†å“ªä¸€æ¡ä¾æ®ã€‚
2. **é€»è¾‘æ¨å¯¼**ï¼šä¸è¦ç®€å•å¤åˆ¶æ³•æ¡ï¼Œè¦è§£é‡Šè¯¥æ³•æ¡å¦‚ä½•é€‚ç”¨äºç”¨æˆ·çš„å…·ä½“é—®é¢˜ã€‚
3. **ä¸¥è°¨æ€§**ï¼šå¦‚æœæä¾›çš„ã€æ³•å¾‹æ¡æ–‡ä¾æ®ã€‘ä¸­å®Œå…¨æ²¡æœ‰ç›¸å…³ä¿¡æ¯ï¼Œè¯·ç›´æ¥å›ç­” 'SEARCH_NEEDED'ã€‚
4. **æ³•å¾‹ç”¨è¯­**ï¼šä½¿ç”¨ä¸“ä¸šã€å®¢è§‚çš„æ³•å¾‹æœ¯è¯­ï¼Œé¿å…å£è¯­åŒ–ã€‚
5. **æ³•å¾‹ä¼¦ç†**ï¼šè§¦çŠ¯ä¼¦ç†é“å¾·ã€å…¬æ°‘éšç§ã€è¿åå®ªæ³•çš„é—®é¢˜ç¦æ­¢å›ç­”ã€‚

ã€æ³•å¾‹æ¡æ–‡ä¾æ®ã€‘ï¼š
{context}"""),
        ("human", "{input}")
    ])

    chain = qa_prompt | llm
    response = chain.invoke({"context": context_text, "input": query}).content

    # 4. åˆ¤æ–­æ˜¯å¦éœ€è¦è”ç½‘è¡¥å……
    if "SEARCH_NEEDED" in response:
        return run_web_search(query)
    else:
        # æ ¼å¼åŒ–è¾“å‡ºï¼Œå¢å¼ºâ€œå·²å­¦ä¹ æœ¬åœ°çŸ¥è¯†â€çš„æ„ŸçŸ¥
        final_answer = f"{response}\n\n---\n** æœ¬æ¬¡å›ç­”åŸºäºä»¥ä¸‹æ³•å¾‹æ¡æ–‡ï¼š**\n"
        sources = list(set([os.path.basename(d.metadata.get("source", "æ³•å¾‹æ–‡æ¡£")) for d in docs]))
        for s in sources:
            final_answer += f"- {s}\n"
        return final_answer


def run_web_search(query):
    """æ‰§è¡Œè”ç½‘æœç´¢å¹¶æ€»ç»“"""
    try:
        # 1. æ‰§è¡Œæœç´¢
        search_results = search_tool.invoke(query)

        # 2. è®© AI æ€»ç»“æœç´¢ç»“æœ
        summary_prompt = ChatPromptTemplate.from_messages([
            ("system",
             "ä½ æ˜¯ä¸€ååŠ©æ‰‹ã€‚ç”¨æˆ·çš„é—®é¢˜åœ¨æœ¬åœ°æ–‡æ¡£ä¸­æœªæ‰¾åˆ°ç­”æ¡ˆï¼Œç³»ç»Ÿå·²è‡ªåŠ¨è”ç½‘æœç´¢ã€‚è¯·æ ¹æ®ä»¥ä¸‹æœç´¢ç»“æœå›ç­”ç”¨æˆ·é—®é¢˜ã€‚"),
            ("human", "ã€æœç´¢ç»“æœã€‘ï¼š\n{results}\n\nã€ç”¨æˆ·é—®é¢˜ã€‘ï¼š{query}")
        ])
        chain = summary_prompt | llm
        answer = chain.invoke({"results": search_results, "query": query}).content

        return f"{answer}\n\n---\n**ğŸŒ ç­”æ¡ˆæ¥æºï¼šè”ç½‘æ£€ç´¢**"
    except Exception as e:
        return f"âš ï¸ æœ¬åœ°æ–‡æ¡£æœªæ‰¾åˆ°ç­”æ¡ˆï¼Œä¸”è”ç½‘æœç´¢å¤±è´¥ï¼š{e}"


# åŠŸèƒ½äº”ï¼šæ–‡çŒ®é˜…è¯»
# 1. æ–‡æ¡£åŠ è½½å™¨
def load_document(file_path):
    """æ ¹æ®æ–‡ä»¶åç¼€è‡ªåŠ¨é€‰æ‹©åŠ è½½å™¨ (æ”¯æŒ PDF, DOCX, TXT)"""
    if file_path.endswith(".txt"):
        # encoding="utf-8" é˜²æ­¢ä¸­æ–‡ä¹±ç 
        return TextLoader(file_path, encoding="utf-8").load()
    elif file_path.endswith(".docx"):
        return Docx2txtLoader(file_path).load()
    elif file_path.endswith(".pdf"):
        return PyPDFLoader(file_path).load()
    else:
        raise ValueError(f"ä¸æ”¯æŒçš„æ–‡ä»¶æ ¼å¼: {file_path}")

# 2. æ–‡æ¡£å¤„ç†å™¨
def process_document(uploaded_file):
    """
    å¤„ç†ä¸Šä¼ æ–‡ä»¶ï¼šä¿å­˜ä¸´æ—¶æ–‡ä»¶ -> åŠ è½½ -> æ‘˜è¦ -> å‘é‡åŒ–(DashScope)
    """
    # 1. è·å–æ–‡ä»¶åç¼€å¹¶ä¿å­˜
    # uploaded_file.name è·å–æ–‡ä»¶åï¼Œsplitext åˆ†ç¦»åç¼€
    file_ext = os.path.splitext(uploaded_file.name)[1].lower()

    if file_ext not in [".pdf", ".docx", ".txt"]:
        raise ValueError("ä»…æ”¯æŒ .pdf, .docx, .txt æ ¼å¼")

    # åˆ›å»ºä¸´æ—¶æ–‡ä»¶
    with tempfile.NamedTemporaryFile(delete=False, suffix=file_ext) as tmp_file:
        tmp_file.write(uploaded_file.getvalue())
        tmp_path = tmp_file.name

    try:
        # 2. è°ƒç”¨ç»Ÿä¸€åŠ è½½é€»è¾‘
        docs = load_document(tmp_path)
        # 3. ç”Ÿæˆæ‘˜è¦ (æˆªå–å‰15kå­—ç¬¦ä»¥é˜²è¶…é•¿)
        full_text = "\n\n".join([d.page_content for d in docs])
        if not full_text.strip():
            raise ValueError("æ–‡æ¡£å†…å®¹ä¸ºç©ºæˆ–æ— æ³•è¯†åˆ«")
        summary_prompt = ChatPromptTemplate.from_messages([
            ("system", "ä½ æ˜¯ä¸€åå­¦æœ¯åŠ©æ‰‹ã€‚è¯·é˜…è¯»ä»¥ä¸‹æ–‡çŒ®å†…å®¹ï¼Œç”Ÿæˆä¸€ä»½ç»“æ„åŒ–æ‘˜è¦ã€‚\n"
                       "è¦æ±‚åŒ…å«ï¼š\n"
                       "1. æ ¸å¿ƒè§‚ç‚¹ (Core Argument)\n"
                       "2. ä¸»è¦è®ºæ®/æ–¹æ³• (Methodology)\n"
                       "3. ç ”ç©¶ç»“è®º (Conclusion)\n"
                       "4. åˆ›æ–°ç‚¹æˆ–å±€é™æ€§ (è‹¥æœ‰)\n"
                       "å­—æ•°æ§åˆ¶åœ¨ 600 å­—ä»¥å†…ã€‚"),
            ("human", "ã€æ–‡çŒ®å†…å®¹ç‰‡æ®µã€‘\n{text}")
        ])
        summary = (summary_prompt | llm).invoke({"text": full_text[:15000]}).content
        # 4. æ„å»ºä¸“å±å‘é‡åº“ (ä½¿ç”¨å…¨å±€å®šä¹‰çš„ embeddings)
        text_splitter = RecursiveCharacterTextSplitter(chunk_size=800, chunk_overlap=100)
        splits = text_splitter.split_documents(docs)
        # ç›´æ¥ä½¿ç”¨ DashScope çš„ embeddings å¯¹è±¡
        vectorstore = FAISS.from_documents(splits, embeddings)
        return summary, vectorstore

    except Exception as e:
        raise e
    finally:
        # æ¸…ç†ä¸´æ—¶æ–‡ä»¶
        if os.path.exists(tmp_path):
            os.remove(tmp_path)



# 3ï¼šæ–‡çŒ®é—®ç­”
def ask_document_with_reasoning(query, vectorstore):
    """
    æ–‡çŒ®é˜…è¯»ä¸“ç”¨é—®ç­”å‡½æ•°ï¼š
    1. æ£€ç´¢æ›´å¤šä¸Šä¸‹æ–‡ (k=5)
    2. ä½¿ç”¨æ€ç»´é“¾ Prompt è®©æ¨¡å‹æ·±åº¦æ€è€ƒ
    3. è¿”å›ç­”æ¡ˆå¹¶é™„å¸¦åŸæ–‡å¼•ç”¨
    """
    if not vectorstore:
        return "è¯·å…ˆä¸Šä¼ æ–‡çŒ®ã€‚"

    # 1. æ£€ç´¢ï¼šç¨å¾®å¢åŠ  k å€¼ä»¥è·å–æ›´å¤šä¸Šä¸‹æ–‡ï¼Œä¾¿äºç»¼åˆåˆ†æ
    retriever = vectorstore.as_retriever(search_kwargs={"k": 5})
    docs = retriever.invoke(query)

    # æ•´ç†ä¸Šä¸‹æ–‡ï¼Œä¿ç•™é¡µç /æ¥æºä¿¡æ¯ä»¥ä¾¿å¼•ç”¨
    context_parts = []
    for i, d in enumerate(docs):
        # æˆªå–æ¯ä¸ªç‰‡æ®µçš„å‰200å­—å±•ç¤ºåœ¨æ¥æºä¸­ï¼Œé˜²æ­¢è¿‡é•¿
        content_preview = d.page_content.replace('\n', ' ')
        source_info = f"[ç‰‡æ®µ{i + 1}] {content_preview}..."
        context_parts.append(d.page_content)

    context_text = "\n\n".join(context_parts)

    # 2. æ„å»ºæ€ç»´é“¾ Prompt (Chain of Thought)
    # è¿™é‡Œçš„ System Prompt æ˜¯è®©æ¨¡å‹"æ€è€ƒ"çš„å…³é”®
    system_prompt = """ä½ æ˜¯ä¸€åä¸¥è°¨çš„å­¦æœ¯ç ”ç©¶åŠ©æ‰‹ã€‚è¯·é˜…è¯»ä¸‹æ–¹çš„ã€å‚è€ƒæ–‡çŒ®ç‰‡æ®µã€‘ï¼Œå¹¶å›ç­”ç”¨æˆ·çš„é—®é¢˜ã€‚

è¯·æŒ‰ç…§ä»¥ä¸‹æ­¥éª¤æ€è€ƒï¼ˆChain of Thoughtï¼‰ï¼š
1. **ä¿¡æ¯å®šä½**ï¼šåœ¨å‚è€ƒæ–‡çŒ®ä¸­æ‰¾åˆ°ä¸é—®é¢˜ç›¸å…³çš„å…·ä½“å¥å­æˆ–æ®µè½ã€‚
2. **é€»è¾‘åˆ†æ**ï¼šç»“åˆä¸Šä¸‹æ–‡ç†è§£è¿™äº›ä¿¡æ¯çš„å«ä¹‰ï¼Œæ’é™¤æ— å…³å¹²æ‰°ã€‚
3. **ç­”æ¡ˆç”Ÿæˆ**ï¼šåŸºäºåŸæ–‡äº‹å®ç”Ÿæˆç­”æ¡ˆï¼Œä¸è¦ç¼–é€ ã€‚å¦‚æœåŸæ–‡ä¸­æ²¡æœ‰æåŠï¼Œè¯·æ˜ç¡®è¯´æ˜â€œæ–‡ä¸­æœªæåŠâ€ã€‚
4. **å¼•ç”¨æ ‡æ³¨**ï¼šåœ¨å›ç­”ä¸­é€‚å½“å¼•ç”¨åŸæ–‡çš„å…³é”®è¡¨è¿°ã€‚

ã€å‚è€ƒæ–‡çŒ®ç‰‡æ®µã€‘ï¼š
{context}
"""

    prompt = ChatPromptTemplate.from_messages([
        ("system", system_prompt),
        ("human", "{input}")
    ])

    # 3. ç”Ÿæˆå›ç­”
    chain = prompt | llm
    response = chain.invoke({"context": context_text, "input": query}).content

    # 4. æ ¼å¼åŒ–è¾“å‡ºï¼šç­”æ¡ˆ + å‚è€ƒæ¥æº
    # æå–æ¥æºæ–‡ä»¶å
    sources = list(set([os.path.basename(d.metadata.get("source", "å½“å‰æ–‡æ¡£")) for d in docs]))
    source_str = "\n".join([f"- {s}" for s in sources])

    # å¯ä»¥åœ¨è¿™é‡ŒæŠŠæ£€ç´¢åˆ°çš„å…·ä½“ç‰‡æ®µä¹ŸæŠ˜å æ˜¾ç¤ºå‡ºæ¥ï¼Œå¢å¼ºâ€œæœ‰æ®å¯æŸ¥â€çš„æ„Ÿè§‰
    detailed_sources = "\n".join([f"> **ç‰‡æ®µ {i + 1}**: {d.page_content[:100]}..." for i, d in enumerate(docs)])

    final_output = f"{response}\n\n---\n**ğŸ“š æ€è€ƒä¾æ®**ï¼š\n{detailed_sources}"

    return final_output


# å¯¼å‡ºå†å²è®°å½•è¾…åŠ©å‡½æ•°
def convert_history_to_md(chat_history, summary=""):
    """å¯¹è¯ç±»å†å²è½¬Markdown (é—®ç­”ã€æ–‡çŒ®é˜…è¯»)"""
    md_text = f"# âš–ï¸ å¯¹è¯è®°å½•å­˜æ¡£ - {time.strftime('%Y-%m-%d %H:%M:%S')}\n\n"
    if summary:
        md_text += f"## ğŸ“„ æ‘˜è¦\n{summary}\n\n---\n\n"
    md_text += "## ğŸ’¬ å¯¹è¯è¯¦æƒ…\n"
    for msg in chat_history:
        role_icon = "ğŸ‘¤" if msg["role"] == "user" else "ğŸ¤–"
        md_text += f"### {role_icon} {msg['role']}:\n{msg['content']}\n\n"
    return md_text


def generate_universal_md(data_list, mode):
    """éå¯¹è¯ç±»å†å²è½¬Markdown (ç¿»è¯‘ã€æ¡ˆä¾‹ã€æ³•è€ƒ)"""
    md = f"# âš–ï¸ {mode}è®°å½•å­˜æ¡£ - {time.strftime('%Y-%m-%d %H:%M:%S')}\n\n"
    for idx, item in enumerate(data_list, 1):
        md += f"## è®°å½• {idx}\n"
        if mode == "Translation":
            md += f"**åŸæ–‡**:\n{item['input']}\n\n**è¯‘æ–‡**:\n{item['output']}\n"
        elif mode == "CaseAnalysis":
            md += f"**æ¡ˆæƒ…**:\n{item['input']}\n\n**åˆ†ææŠ¥å‘Š**:\n{item['output']}\n"
        elif mode == "Exam":
            md += f"**ç§‘ç›®**: {item['subject']}\n**é¢˜ç›®**:\n{item['q']}\n\n**è§£æ**:\n{item['a']}\n"
        md += "\n---\n"
    return md


def render_history_ui(session_key, mode_name, md_type="universal"):
    """ç»Ÿä¸€æ¸²æŸ“å†å²è®°å½•çš„ä¸‹è½½ä¸æ¸…ç©ºæŒ‰é’®"""
    if session_key in st.session_state and st.session_state[session_key]:
        st.divider()
        st.caption(f"ğŸ“Š {mode_name} - å†å²è®°å½•ç®¡ç†")
        c1, c2 = st.columns([1, 1])
        with c1:
            data = st.session_state[session_key]
            if md_type == "universal":
                # æ˜ å°„ mode_name åˆ°å†…éƒ¨ mode å­—ç¬¦ä¸²
                internal_mode = "Translation" if "ç¿»è¯‘" in mode_name else "CaseAnalysis" if "æ¡ˆä¾‹" in mode_name else "Exam"
                md_text = generate_universal_md(data, internal_mode)
            else:
                # å¯¹è¯ç±»ç›´æ¥ä¼ åˆ—è¡¨
                md_text = convert_history_to_md(data)

            st.download_button(
                label=f"ğŸ“¥ å¯¼å‡º{mode_name}è®°å½•",
                data=md_text,
                file_name=f"{mode_name}_History.md",
                mime="text/markdown"
            )
        with c2:
            if st.button(f"ğŸ—‘ï¸ æ¸…ç©º{mode_name}è®°å½•", key=f"clear_{session_key}"):
                st.session_state[session_key] = []
                st.rerun()

        # ç®€å•å±•ç¤ºå†å²æ¡ç›®æ•°
        if md_type == "universal":
            with st.expander(f"æŸ¥çœ‹å†å²åˆ—è¡¨ ({len(st.session_state[session_key])}æ¡)"):
                for i, item in enumerate(reversed(st.session_state[session_key])):
                    st.text(f"è®°å½• {len(st.session_state[session_key]) - i}")
                    # ç®€ç•¥æ˜¾ç¤ºå†…å®¹
                    if 'input' in item:
                        st.caption(item['input'][:50] + "...")
                    elif 'q' in item:
                        st.caption(item['q'][:50] + "...")
                    st.divider()

# --- 4. Streamlit å‰ç«¯ç•Œé¢ ---

st.title("âš–ï¸æ³•åŠ©æ‰‹")

# ä¾§è¾¹æ 
MENU_OPTIONS = [
    "æ³•å¾‹æ–‡æœ¬ç¿»è¯‘",
    "æ¡ˆä¾‹æ™ºèƒ½åˆ†æ",
    "æ³•å¾‹çŸ¥è¯†é—®ç­”",
    "æ–‡çŒ®é˜…è¯»",
    "æ³•è€ƒå¤‡è€ƒ"
]
option = st.sidebar.radio("åŠŸèƒ½å¯¼èˆª", MENU_OPTIONS)
st.sidebar.markdown("---")

if st.sidebar.button("ğŸ—‘ï¸ æ¸…ç©ºå½“å‰ä¼šè¯", type="primary"):
    # æ¸…é™¤æ‰€æœ‰ Session State
    st.session_state.clear()
    # å¼ºåˆ¶åˆ·æ–°é¡µé¢
    st.rerun()

# --- åŠŸèƒ½é€»è¾‘ ---
# 1. æ³•å¾‹æ–‡æœ¬ç¿»è¯‘
if option == "æ³•å¾‹æ–‡æœ¬ç¿»è¯‘":
    st.header("ä¸“ä¸šæ³•å¾‹æ–‡æœ¬ç¿»è¯‘")

    # --- 1. è¾“å…¥åŒºåŸŸé€‰æ‹© ---
    input_method = st.radio("é€‰æ‹©è¾“å…¥æ¥æº", ["âœï¸ æ‰‹åŠ¨è¾“å…¥æ–‡æœ¬", "ğŸ“„ ä¸Šä¼ æ–‡æ¡£ (PDF/Docx)"], horizontal=True)

    final_text = ""  # å­˜å‚¨æœ€ç»ˆå¾…ç¿»è¯‘çš„å†…å®¹

    if input_method == "ğŸ“„ ä¸Šä¼ æ–‡æ¡£ (PDF/Docx)":
        uploaded_file = st.file_uploader("è¯·ä¸Šä¼ å¾…ç¿»è¯‘æ–‡ä»¶", type=["pdf", "docx"])
        if uploaded_file:
            with tempfile.NamedTemporaryFile(delete=False, suffix=os.path.splitext(uploaded_file.name)[1]) as tmp:
                tmp.write(uploaded_file.getvalue())
                tmp_path = tmp.name

            try:
                if uploaded_file.name.endswith(".pdf"):
                    loader = PyPDFLoader(tmp_path)
                else:
                    loader = Docx2txtLoader(tmp_path)
                docs = loader.load()
                final_text = "\n".join([d.page_content for d in docs])
                st.success(f"âœ… æ–‡ä»¶è§£ææˆåŠŸï¼Œå…±æå– {len(final_text)} å­—")
            finally:
                if os.path.exists(tmp_path): os.remove(tmp_path)
    else:
        final_text = st.text_area("è¯·åœ¨æ­¤ç²˜è´´å¾…ç¿»è¯‘æ–‡æœ¬", height=200, placeholder="åœ¨æ­¤è¾“å…¥...")

    # --- 2. ç¿»è¯‘è®¾ç½® ---
    st.write("")
    c1, c2 = st.columns(2)
    with c1:
        target_lang = st.selectbox("ç›®æ ‡è¯­è¨€", ["è‹±æ–‡", "ä¸­æ–‡"])
    with c2:
        use_rag = st.checkbox("å¯ç”¨æœ¯è¯­åº“å¢å¼º", value=True)

    # --- 3. æ‰§è¡Œç¿»è¯‘ ---
    if st.button("å¼€å§‹ç¿»è¯‘", type="primary", use_container_width=True):
        if not final_text.strip():
            st.warning("å†…å®¹ä¸ºç©ºï¼Œè¯·è¾“å…¥æ–‡æœ¬æˆ–ä¸Šä¼ æ–‡ä»¶ã€‚")
        else:
            with st.spinner("æ³•åŠ©æ‰‹æ­£åœ¨æ£€ç´¢æœ¯è¯­åº“å¹¶ç”Ÿæˆè¯‘æ–‡..."):
                # æ£€ç´¢ç›¸å…³æœ¯è¯­ (RAG)
                context_terms = ""
                if use_rag and term_vectorstore:
                    # æ£€ç´¢æœ€ç›¸å…³çš„20æ¡æœ¯è¯­å¯¹
                    search_docs = term_vectorstore.similarity_search(final_text[:500], k=20)
                    context_terms = "\n".join([d.page_content for d in search_docs])

                # æ„å»ºç¿»è¯‘é“¾
                prompt = ChatPromptTemplate.from_messages([
                    ("system", """ä½ æ˜¯ä¸€ä½èµ„æ·±çš„æ³•å¾‹ç¿»è¯‘ä¸“å®¶ã€‚è¯·å°†ä»¥ä¸‹æ–‡æœ¬ç¿»è¯‘ä¸º{lang}ã€‚

                    ã€ç¿»è¯‘å‡†åˆ™ã€‘ï¼š
                    1. å‚è€ƒæä¾›çš„ã€æœ¯è¯­å¯¹ç…§è¡¨ã€‘ï¼Œç¡®ä¿æ ¸å¿ƒè¯æ±‡ä¸“ä¸šä¸”ç»Ÿä¸€ã€‚
                    2. ä½¿ç”¨æ­£å¼æ³•å¾‹æ–‡ä½“ï¼Œä¿æŒæ¡æ¬¾ç¼–å·å’Œæ ¼å¼ã€‚

                    ã€æœ¯è¯­å¯¹ç…§è¡¨ã€‘ï¼š
                    {context}"""),
                    ("human", "å¾…ç¿»è¯‘æ–‡æœ¬ï¼š\n{text}")
                ])

                chain = prompt | llm
                res = chain.invoke({
                    "lang": target_lang,
                    "context": context_terms,
                    "text": final_text
                })

                st.subheader("ğŸ“‘ ç¿»è¯‘ç»“æœ")
                st.success(res.content)

                st.download_button("ä¸‹è½½è¯‘æ–‡", res.content, file_name="translated_legal.txt")


# 2. æ¡ˆä¾‹æ™ºèƒ½åˆ†æ
elif option == "æ¡ˆä¾‹æ™ºèƒ½åˆ†æ":
    st.header("æ¡ˆä¾‹æ¡ˆæƒ…åˆ†æ")

    if "case_history" not in st.session_state:
        st.session_state.case_history = []

    case_input = st.text_area("è¯·è¾“å…¥æ¡ˆæƒ…äº‹å®", height=200)

    if st.button("ç”Ÿæˆåˆ†ææŠ¥å‘Š"):
        if not case_input.strip():
            st.warning("è¯·è¾“å…¥æ¡ˆæƒ…")
        else:
            with st.spinner("æ’°å†™æŠ¥å‘Š..."):
                # ä¼˜åŒ–ï¼šåªè°ƒç”¨ä¸€æ¬¡ AI
                res = case_analysis(case_input)
                st.markdown(res)
                st.session_state.case_history.append({"input": case_input, "output": res})

    # å†å²è®°å½•
    render_history_ui("case_history", "æ¡ˆä¾‹åˆ†æ")


# 3. æ³•å¾‹çŸ¥è¯†é—®ç­”
elif option == "æ³•å¾‹çŸ¥è¯†é—®ç­”":
    st.header("æ³•å¾‹çŸ¥è¯†é—®ç­”")
    st.markdown("æ³•åŠ©æ‰‹ä¼šç»“åˆæœ¬åœ°çŸ¥è¯†åº“å›ç­”ä½ çš„æé—®ã€‚")

    with st.form(key="qa_form", clear_on_submit=True):  # clear_on_submit=True å‘é€åæ¸…ç©º
        col1, col2 = st.columns([5, 1])
        with col1:
            user_input = st.text_input("é—®é¢˜", placeholder="è¯·è¾“å…¥ä½ çš„é—®é¢˜...", label_visibility="collapsed")
        with col2:
            submitted = st.form_submit_button("å‘é€")

    if "messages" not in st.session_state:
        st.session_state.messages = []

    # æ¸²æŸ“å†å²è®°å½•
    for msg in st.session_state.messages:
        st.chat_message(msg["role"]).write(msg["content"])

    # é€»è¾‘åˆ¤æ–­ï¼šå¦‚æœç‚¹å‡»äº†æäº¤æŒ‰é’® ä¸” è¾“å…¥æ¡†ä¸ä¸ºç©º
    if submitted and user_input:
        st.session_state.messages.append({"role": "user", "content": user_input})
        st.chat_message("user").write(user_input)

        with st.chat_message("assistant"):
            with st.spinner("æ£€ç´¢ä¸­..."):
                full_res = smart_qa_search(user_input, vectorstore)
                st.markdown(full_res)
                st.session_state.messages.append({"role": "assistant", "content": full_res})

    # å†å²ç®¡ç†
    if st.session_state.messages:
        render_history_ui("messages", "é—®ç­”", md_type="chat")


# 4. æ–‡çŒ®é˜…è¯»
elif option == "æ–‡çŒ®é˜…è¯»":
    st.header("æ–‡çŒ®æ™ºèƒ½é˜…è¯» & æ·±åº¦æ€è€ƒ")

    if "doc_state" not in st.session_state:
        st.session_state.doc_state = {
            "current_file_name": None, "summary": "", "vectorstore": None, "chat_history": []
        }

    # 1. æ–‡ä»¶ä¸Šä¼ 
    uploaded_file = st.file_uploader("ğŸ“‚ ä¸Šä¼ æ–‡çŒ® (PDF/Docx/Txt)", type=["pdf", "docx", "txt"])

    if uploaded_file:
        current_name = uploaded_file.name
        # åªæœ‰æ–‡ä»¶åå˜åŒ–æ—¶æ‰é‡æ–°è§£æ
        if current_name != st.session_state.doc_state["current_file_name"]:
            with st.status("ğŸ” æ­£åœ¨æ·±åº¦é˜…è¯»æ–‡çŒ®...", expanded=True) as status:
                try:
                    st.write("1. æ­£åœ¨æå–æ–‡æœ¬å†…å®¹...")
                    # è°ƒç”¨ä¹‹å‰çš„ process_document å‡½æ•°
                    summary, vs = process_document(uploaded_file)

                    st.write("2. æ­£åœ¨æ„å»ºçŸ¥è¯†ç´¢å¼•...")
                    st.write("3. æ­£åœ¨ç”Ÿæˆå…¨æ–‡æ‘˜è¦...")

                    # æ›´æ–°çŠ¶æ€
                    st.session_state.doc_state.update({
                        "current_file_name": current_name,
                        "summary": summary,
                        "vectorstore": vs,
                        "chat_history": []
                    })
                    status.update(label="âœ… æ–‡çŒ®é˜…è¯»å®Œæˆï¼", state="complete", expanded=False)
                except Exception as e:
                    st.error(f"è§£æå¤±è´¥: {e}")
                    status.update(label="âŒ å¤±è´¥", state="error")

    # 2. æ˜¾ç¤ºæ‘˜è¦
    if st.session_state.doc_state["summary"]:
        with st.expander("ğŸ“„ ç‚¹å‡»æŸ¥çœ‹ã€å…¨æ–‡æ™ºèƒ½æ‘˜è¦ã€‘", expanded=True):
            st.markdown(st.session_state.doc_state["summary"])

    # 3. å¯¹è¯åŒºåŸŸ
    if st.session_state.doc_state["vectorstore"]:
        st.divider()
        st.subheader("ğŸ’¬ åŸºäºæ–‡çŒ®çš„é—®ç­”åŠ©æ‰‹")

        # æ¸²æŸ“å†å²è®°å½•
        for msg in st.session_state.doc_state["chat_history"]:
            st.chat_message(msg["role"]).write(msg["content"])

        # è¾“å…¥æ¡†
        if query := st.chat_input("å‘ AI æé—®å…³äºè¿™ç¯‡æ–‡çŒ®çš„å†…å®¹..."):
            # ç”¨æˆ·æ¶ˆæ¯
            st.session_state.doc_state["chat_history"].append({"role": "user", "content": query})
            st.chat_message("user").write(query)

            # AI å›ç­”
            with st.chat_message("assistant"):
                with st.spinner("ğŸ¤” æ­£åœ¨æ£€ç´¢åŸæ–‡å¹¶æ€è€ƒ..."):
                    # === å…³é”®ä¿®æ”¹ï¼šè°ƒç”¨å¸¦æ€è€ƒé€»è¾‘çš„ä¸“ç”¨å‡½æ•° ===
                    answer = ask_document_with_reasoning(query, st.session_state.doc_state["vectorstore"])

                    st.markdown(answer)
                    st.session_state.doc_state["chat_history"].append({"role": "assistant", "content": answer})

        # åº•éƒ¨åŠŸèƒ½æ 
        if st.session_state.doc_state["chat_history"]:
            st.divider()
            c1, c2 = st.columns([1, 1])
            with c1:
                # å¯¼å‡ºåŠŸèƒ½
                md = convert_history_to_md(
                    st.session_state.doc_state["chat_history"],
                    st.session_state.doc_state["summary"]
                )
                st.download_button("ğŸ“¥ å¯¼å‡ºæœ¬ç¯‡å¯¹è¯è®°å½•", md, "doc_reading.md")
            with c2:
                # æ¸…ç©ºåŠŸèƒ½
                if st.button("ğŸ—‘ï¸ æ¸…ç©ºæœ¬ç¯‡å¯¹è¯"):
                    st.session_state.doc_state["chat_history"] = []
                    st.rerun()

# 5. æ³•è€ƒå¤‡è€ƒ
elif option == "æ³•è€ƒå¤‡è€ƒ":
    st.header("ğŸ“š æ³•è€ƒæ™ºèƒ½åˆ·é¢˜ç³»ç»Ÿ")

    # 1. åŠ è½½é¢˜åº“
    if "exam_db" not in st.session_state:
        if os.path.exists(EXAM_DB_FILE):
            with open(EXAM_DB_FILE, "r", encoding="utf-8") as f:
                st.session_state.exam_db = json.load(f)
        else:
            st.session_state.exam_db = {}

    if not st.session_state.exam_db:
        st.error("æœªæ£€æµ‹åˆ°æœ¬åœ°é¢˜åº“æ–‡ä»¶ (exam_db.json)ã€‚")
        st.info("è¯·ç¡®ä¿å·²è¿è¡Œ `æ™ºèƒ½å‡ºé¢˜.py` ç”Ÿæˆé¢˜åº“ã€‚")
    else:
        # --- åˆå§‹åŒ–å…¨å±€ Session å˜é‡ ---
        if "current_q_index" not in st.session_state: st.session_state.current_q_index = 0
        if "show_exam_answer" not in st.session_state: st.session_state.show_exam_answer = False
        if "ai_exam_analysis" not in st.session_state: st.session_state.ai_exam_analysis = None
        if "exam_history" not in st.session_state: st.session_state.exam_history = []
        if "last_subject" not in st.session_state: st.session_state.last_subject = None

        # 2. ç§‘ç›®é€‰æ‹©
        subjects = list(st.session_state.exam_db.keys())
        selected_sub = st.selectbox("é€‰æ‹©ç»ƒä¹ ç§‘ç›®", subjects)

        # å¦‚æœåˆ‡æ¢äº†ç§‘ç›®ï¼Œè‡ªåŠ¨é‡ç½®çŠ¶æ€
        if st.session_state.last_subject != selected_sub:
            st.session_state.current_q_index = 0
            st.session_state.show_exam_answer = False
            st.session_state.ai_exam_analysis = None
            st.session_state.last_subject = selected_sub

        question_pool = st.session_state.exam_db[selected_sub]

        # 3. é¢˜ç›®æ§åˆ¶å·¥å…·æ 
        t_col1, t_col2 = st.columns([3, 1])
        with t_col1:
            st.caption(f"å½“å‰ç§‘ç›®ï¼š**{selected_sub}** | é¢˜åº“é‡ï¼š{len(question_pool)}")
        with t_col2:
            if st.button("ğŸ² éšæœºæŠ½é¢˜", use_container_width=True):
                # ç¡®ä¿éšæœºæŠ½åˆ°çš„ä¸æ˜¯å½“å‰è¿™ä¸€é¢˜ï¼ˆå¦‚æœé¢˜åº“å¤§äº1é“çš„è¯ï¼‰
                new_idx = st.session_state.current_q_index
                if len(question_pool) > 1:
                    while new_idx == st.session_state.current_q_index:
                        new_idx = random.randint(0, len(question_pool) - 1)
                st.session_state.current_q_index = new_idx
                st.session_state.show_exam_answer = False
                st.session_state.ai_exam_analysis = None
                st.rerun()

        # 4. é¢˜ç›®æ¸²æŸ“åŒºåŸŸ
        if question_pool:
            q_data = question_pool[st.session_state.current_q_index]

            with st.container(border=True):
                st.subheader(f"é¢˜ç›® {st.session_state.current_q_index + 1}")
                st.markdown(f"**{q_data['question_text']}**")

                options = q_data.get('options', [])
                user_choice = None

                if options:
                    # ä½¿ç”¨ç‰¹å®šçš„ Key ç¡®ä¿å•é€‰æ¡†éšé¢˜ç›®ç´¢å¼•åˆ·æ–°
                    user_choice = st.radio(
                        "è¯·é€‰æ‹©ä½ çš„ç­”æ¡ˆï¼š",
                        options,
                        index=None,
                        key=f"radio_{selected_sub}_{st.session_state.current_q_index}"
                    )
                else:
                    st.info("æ­¤é¢˜ä¸ºéé€‰æ‹©é¢˜ï¼ˆä¸»è§‚é¢˜/åˆ¤æ–­é¢˜ï¼‰ã€‚")
                    user_choice = st.text_area("ç­”é¢˜æ€è·¯è®°å½•ï¼š",
                                               key=f"text_{selected_sub}_{st.session_state.current_q_index}")

                st.write("")
                if st.button("æäº¤ç­”æ¡ˆ", type="primary"):
                    if (options and len(options) > 0) and not user_choice:
                        st.warning("è¯·å…ˆé€‰æ‹©ä¸€ä¸ªé€‰é¡¹ï¼")
                    else:
                        st.session_state.show_exam_answer = True

                        std_answer = q_data.get('correct_answer')
                        raw_analysis = q_data.get('analysis', "").strip()

                        # æƒ…å†µ 1ï¼šæ•°æ®åº“ä¸­å®Œå…¨æ²¡æœ‰ç­”æ¡ˆ
                        if not std_answer or not str(std_answer).strip():
                            with st.spinner("åŸçœŸé¢˜æœªåŒ…å«ç­”æ¡ˆï¼Œæ³•åŠ©æ‰‹æ­£åœ¨æ£€ç´¢æœ¬åœ°æ³•æ¡åº“è¿›è¡Œæ·±åº¦è§£æ..."):
                                query = f"é¢˜ç›®ï¼š{q_data['question_text']}\né€‰é¡¹ï¼š{options}\nè¯·ç»™å‡ºæ­£ç¡®ç­”æ¡ˆå’Œè¯¦ç»†æ³•å¾‹è§£æã€‚"
                                ai_res = smart_qa_search(query, vectorstore)
                                st.session_state.ai_exam_analysis = ai_res

                        # æƒ…å†µ 2ï¼šæœ‰ç­”æ¡ˆä½†ã€è§£æä¸ºç©ºã€‘ï¼ˆæ ¸å¿ƒæ”¹è¿›ç‚¹ï¼‰
                        elif not raw_analysis:
                            with st.spinner("æ£€æµ‹åˆ°è§£æç¼ºå¤±ï¼Œæ­£åœ¨æ ¹æ®ç­”æ¡ˆç”Ÿæˆä¸“ä¸šè§£æ..."):
                                complement_prompt = ChatPromptTemplate.from_messages([
                                    ("system",
                                     "ä½ æ˜¯ä¸€åèµ„æ·±æ³•è€ƒè®²å¸ˆã€‚ç”¨æˆ·ä¼šç»™ä½ ä¸€é“é¢˜ç›®ã€é€‰é¡¹ä»¥åŠæ ‡å‡†ç­”æ¡ˆï¼Œè¯·ä½ ç»“åˆä¸­å›½ç°è¡Œæ³•å¾‹æ¡æ–‡ï¼Œç»™å‡ºå‡†ç¡®ã€è¯¦å°½çš„æ³•ç†åˆ†æã€‚"),
                                    ("human", "ã€é¢˜ç›®ã€‘ï¼š{question}\nã€é€‰é¡¹ã€‘ï¼š{options}\nã€æ ‡å‡†ç­”æ¡ˆã€‘ï¼š{answer}")
                                ])
                                chain = complement_prompt | llm
                                ai_res = chain.invoke({
                                    "question": q_data['question_text'],
                                    "options": options,
                                    "answer": std_answer
                                }).content
                                st.session_state.ai_exam_analysis = ai_res

                        # æƒ…å†µ 3ï¼šå·²æœ‰å®Œæ•´è§£æ
                        else:
                            st.session_state.ai_exam_analysis = None

                        # ç»Ÿä¸€è®°å½•åˆ°å†å²è®°å½•ä¸­
                        final_a = st.session_state.ai_exam_analysis if st.session_state.ai_exam_analysis else raw_analysis
                        st.session_state.exam_history.append({
                            "subject": selected_sub,
                            "q": q_data['question_text'],
                            "a": f"æ ‡å‡†ç­”æ¡ˆï¼š{std_answer}\nè§£æï¼š{final_a}"
                        })

                    # --- ä¿®æ”¹åçš„æ˜¾ç¤ºåŒºåŸŸ ---
                if st.session_state.show_exam_answer:
                    st.divider()
                    st.markdown("### ğŸ’¡ ç­”æ¡ˆè§£æ")

                    std_answer = q_data.get('correct_answer')

                    # åˆ¤å®šç”¨æˆ·é€‰æ‹©å¯¹é”™
                    if std_answer and user_choice:
                        if str(std_answer) in str(user_choice):
                            st.success("ğŸ‰ å›ç­”æ­£ç¡®ï¼")
                        else:
                            st.error(f"âŒ å›ç­”é”™è¯¯ã€‚æ ‡å‡†ç­”æ¡ˆæ˜¯ï¼š{std_answer}")

                    # ä¼˜å…ˆæ˜¾ç¤º AI è¡¥å…¨çš„è§£æï¼Œå¦‚æœæ²¡æœ‰åˆ™æ˜¾ç¤ºåŸå§‹è§£æ
                    if st.session_state.ai_exam_analysis:
                        st.info("**ã€æ³•åŠ©æ‰‹æ·±åº¦è§£æã€‘**")
                        st.markdown(st.session_state.ai_exam_analysis)
                    else:
                        st.info("**ã€çœŸé¢˜è§£æã€‘**")
                        st.markdown(q_data.get('analysis') or "æš‚æ— è§£æ")

    render_history_ui("exam_history", "æ³•è€ƒç»ƒä¹ è®°å½•")
